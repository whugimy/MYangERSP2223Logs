# MYangERSP2223Logs
My logs for ERSP 2022-2023

# Week 7
- [x] Peer Review Monday In-class submit
- [x] Complete reflection 3
- [x] Look into grad student interview
- [ ] Read code in github repo
- [x] Everyone has tested the example code on Colab
- [x] Team meeting with Chinmay on Nov 10, 6pm
- [x] Team meeting with just us afterwards
- [x] Schedule meeting on Friday, ask about final task for this quarter (visualization technique)

## Talk Notes Friday 3:40pm
- Met Arthur for the first time, he's heading back to Brazil.
- Arthur gave a full presentation on Trustee, including details on architecture and some use-cases he and Roman have looked into
- As it turns out, networking datasets are even worse than the paper makes them seem. I didn't have this discussion with anyone, but general sentiment was that any synthetic dataset, and most datasets released in last few years (in networking) are definitely faulty
- Arthur had time to present 3 use-cases where he applied Trustee.
- Overall conclusion was that datasets in the networking space are very bad
- Some discussion about issues with their process:
  - Trustee trains multiple decision trees from the black-box info.
  - The final decision tree is the one with "max mean agreement"
  - He takes every pair of decision trees and calculates their agreement (some numeric measurement of similarity).
  - For each decision tree, calculate its average agreement with all other decision trees
  - The tree with the highest average is the final decision tree.
  - The issue is, any of those valid decision trees with high enough fidelity to the black-box model offers some insight into the potential decision-making process of the black-box.
  - In essence, just because a decision tree has low mean agreement, doesn't mean it doesn't offer insight into the black-box model.
  - Potential long-term project is to further look into knowledge distillation (https://en.wikipedia.org/wiki/Knowledge_distillation) algorithms and apply those to see if we can find a better algorithm. Arthur did emphasize that this will be hard for undergrads (but I think we could totally do it!)
- I also asked Arthur and Roman a bit about more short-term goals:
  - Currently, the trust report that trustee outputs is printed to console. This is definitely suboptimal
  - An immediate doable task is to optimize the output in such a way that it outputs some `.html` file, something similar to what Jacoco line coverage or pitest mutation might produce (Abel will know what this is from CMPSC 156)
  - This file would be navigable, it would contain a ton of graphs that would be relevant to an analyst looking at the results
- My thoughts: The presentation offered a lot more insight into how Trustee is a tool to analyze data more than anything else. A lot of the more specific conclusions that Arthur and Roman reached with Trustee were errors in the data. Small oversights, like how all messages from attackers contained a similar feature (they all came from the same country or all came from out of network) resulted in shortcut learning, even though the presence of that feature logically doesn't always mean "attack" (just because it came from outside doesn't mean it's harmful). We didn't have any plans for the long-term work
- Final tidbit: there's a tool called wireshark that's very useful in learning some basics about packets. It can take some of your own computer's network traffic then explain what some of the information means. Download here: https://www.wireshark.org/
  - Roman called this "the most important networking tool"

## Meeting Notes Thursday 7pm
- We haven't done much so far due to midterms
- Need to work on proposal
- Need to set up meeting on Friday, maybe cancelled due to Holiday?
- Overall, need to focus on getting proposal goals and concrete timeline
- Also need to proofread the existing sections and cut down (some final proposals from last year have less words than ours)
- Brief discussion regarding code in repo, it's about 800 lines of python, most of which is documentation, lost of scikit-learn related info, not too difficult to go through

## Grad Student Interview
- Considering asking my 165A ta, otherwise will join Aditi for her interview
- If joining Aditi, will be sometime next Thursday?

# Week 6
- [x] Study for and finish CMPSC midterms!!
- [x] No meeting this week, everyone is doing midterms.
- [x] Attend monday's github activity

# Week 5
- [x] Continue testing Trustee library this week, ideally write down some notes about the process
- [x] Complete draft of relevant works section of paper
- [x] Read another, related paper
- [x] Finish TA Eval and and mid quarter eval
- [x] Group Meeting on Friday at 4 (over zoom)

## Friday Meeting
- Went over some of the examples over Zoom
- Tried logging into SNL Lab servers (everyone else's looks fine, my account seems to have some issue with ssh key, will get back to Roman on that)
- Tried some of the example notebooks on Google Colab
- Added a few lines to get visuals on the final decision tree and regression result

```
from IPython.display import Image
import pydotplus
graph = pydotplus.graph_from_dot_data(dot_data)  
Image(graph.create_png())
```

## Notes
- Draft of relevant works section is going well, we split the work into a few different sections that each person could work on on their own, but later on we'll have to ensure that we properly transition between each section.

## Notes for paper: The Cross-evaluation of Machine Learning-based Network Intrusion Detection Systems
https://arxiv.org/abs/2203.04686
### Summary
The paper describes an effort to train and evaluate an ML solution for Network Intrusion Detection Systems (NIDS). Because the networking space is generally lacking in regards to available, high-quality training data, the authors have produced a workaround via a superior evaluation system that they call XeNIDS. Overall, XeNIDS more effectively combines multiple existing training datasets to create additional malicious network data for training.

### How paper is related
This paper attempts to solve the same problem (poor trustworthiness of ML applied to networking) with a similar method (more close evaluation of the model). The paper's discussed product does differ greatly in methodology, but is overall a good example of how another group has approached the same problem with a similar paradigm. The techniques used in this paper could be stacked on top of the approach Trustee uses to potentially  further improve trustworthiness.

### Why I chose the paper
As aforementioned, this paper addresses the same issue with a very similar mindset: evaluating the models due to the lack of availability of high quality data. In this case, the paper uses existing training data to further create training data to evaluate the model with (I guess it is fairly different), but it was interesting to see another group approach the same problem with the same understanding that there's no easy way to create more data.

# Week 4
## Goals
- [x] Schedule meeting with Roman and Professor Gupta (meeting occurred on Friday in HFH, Oct 21)
- [x] Read Trustee paper
- [x] Test Trustee library with scikit learn on Sunday

## Meeting Notes
- Everyone should have paper read and begun testing Trustee code by end of next week.
- If anyone has questions, please compile them prior to the meeting or just ask directly on slack so that either they can be answered quickly, or they can be answered together during a meeting.

# Week 3
## Goals
- [x] Write goals 
- [x] Choose topic for teaching assignment due Monday Oct 17
- [x] Meeting with grad student advisor (Roman) on Friday (3pm on Zoom)

## Teaching Assignment 10/12/2022
Chosen topic: black-box vs white-box AI models

### What is a black-box model?
- Black-box refers to any model we can't logically understand.
- Example: neural networks
- We can see mathetmatically how neural networks "work"
- Nodes in the network receive values based on weights and biases of each edge
- Can show image of a neural network here
- Weights represent "connectivity strength" between two nodes, biases are not affected by previous layers
- Complex math involved in actual training process, but we can sort of understand the structure of a simple network
- Black-box aspect comes into play because we aren't quite sure why these decision are made
- Different # of layers can still result in relatively similar success rates, so what is each node doing?
- Use example of number identification: we can look at number 9, 3, and identify features of it in our head
- We can't easily see how a neural network is doing this classification
- We can look at numbers, weights, biases, etc. but ultimately we have no clue why a decision is being made, only that it is and that these numbers lead to it.
- We understand the structure holding the numbers, but we can't get any meaningful logic about what the numbers mean

### What is a white-box model?
- White-box refers to models whose inner workings we not only see, but are able to intuitively understand
- I park my bike in front of library and head in. When I come back, bike is gone. What likely happened? My bike became sentient, a police officer took my bike, my bike got stolen. Probablistically, knowing UCSB, my bike was stolen.
- We make Bayesian decision every day (previous events lead us to consider decisions)
- Something like Bayesian classifier does same thing with large sources of data
- As a result, even though we there's still a lot of math involved in a Bayesian classifier, we can identify what variables affect other ones
- Borrow 165A HW1
- Alternative example: decision tree, linear regression
- Not all white-box models are at same level of interpretability
- Still, unlike black-box, we are able to at least somewhat understand the process of the AI's decision making. 

### Sources:
- https://www.frontiersin.org/articles/10.3389/fpsyt.2020.551299/full
- https://machine-learning.paperspace.com/wiki/weights-and-biases

## Meeting Notes 10/14/2022 3pm
Present: everyone + Roman

Machine Learning is about data, try to find internal rules within the data to create some black/white box model

Black box model cannot be understood (i.e. neural network)
White box model can be understood, can be segmented and viewed more closely (i.e. decision tree)

Most of the time, model is bad because data is bad for multitude of reasons

Trustee is an improvement of existing ideas where it can take a model and create a decision tree. 

Take data to create black box model
Take the results to other model that will produce decision tree
Decision tree attempts to mimic the original black box model
The decision tree can then be viewed and analyzed for biases in data

Message Roman in Slack if we have any questions.

Tasks:
- [ ] Read trustee paper with focus on abstract, introduction, related works (about 3 pages)
- [ ] Look through the repository's website
- [ ] Read trustee code library on Github
- [ ] Progress check on Wednesday or Thursday (Oct 19-20)
- [ ] Schedule follow-up meeting

# Week 2

## Goals
- [x] Read Unicorn research paper
- [x] Have first research group meeting / ~~get response on when mentor meetings will occur~~ meeting will occur on 4PM PST October 7
- [x] Get link to logs on the CS110 website
- [x] Reading Log Pass 2
- [x] Decide on teaching topic (DP)

## Research Meeting Notes:
Visited Professor Gupta's office on 5th floor of HFH to discuss initial project, discussed a bit about Unicorn, but more interesting topic was other "trust" based program that can somehow turn black box AI artifacts into decision trees? Initial thoughts are that there's no way in hell that works well (especially not with any CNN for image recog, I just can't see how that could be a decision tree), but if it does, I will be EXTREMELY impressed. We were told of an open source python package that can be used on Colab, so I will probably test it out next weekend (Oct 15). I will probably try a simple Bayes classifier and the 10x10 pixel number recognition.

## Unicorn Research Paper Notes:
### Abstract:
Existing ML solutions for networking problems are not being implemented; this is because these ML artifacts are often unreliable. The paper makes the claim that these ML artifacts have poor quality due to their training data: the process of collecting data for ML is described as "one-off," "fragmented," and with two key issues: it is too difficult to compile data from different sources to use as training material for the same problem, and simultaneously too difficult to apply data from a single source to mutliple problems. The paper proposes a solution called "Unicorn," which sreamlines the process of collecting data from one source for multiple problems.

### Introduction:
Network operators have, at any point, only a partial understanding of the state of their network, and they constantly need to make decisions to solve some arbitrary "problems." There seems to be industry interest in applying ML techniques to better make these decisions, but existing ML solutions are not being applied. This is because the industry feels wary of ML's innate "black-box" nature. The "black-box" issue refers to our lack of understanding regarding neural networks. Although we can use them, we have almost no insight into how they actually make decisions. The ML artifacts created with given data are simply too complicated for a human to understand. Industry members must then rely solely on the algorithm's results to determine its feasibility. In order for the algorithm to be "trustworthy," it must be able to adapt to and properly process information outside of its training set. Existing ML solutions struggle to do so, as they are often trained with data from one, specific source, and often do not correctly generalize to external data.

Everything beyond the background on the problem being solved describes Unicorn's specifications and how it would help solve the above problems. However, I don't have any technical understanding of ML models or data collection pipelines as they are described in the paper.

### Experimental Process:
The paper describes the process of implementing a Unicorn prototype. The paper describes Unicorn's implementation being split into a frontend and backend. The frontend serves as an interface for the user to select and configure data-collection pipelines. An operation titled "execute" will connect these pipelines to Unicorn's backend and begin processing. Unicorn's backend consists of the following components: 
1. Connectivity-manager: connects Unicorn's front and backend
2. Compiler: translates data-collection pipeline's general data into specific forms depending on what problem the artifact will be trained for, then processes the data (not sure how)
3. Runtime: serves as sort of a general OS that handles queries and displays runtime info
4. Datastore: provides memory for the runtime to manage

### Background:
The paper's background and motivation segment discusses two core concepts: the "trust problem" in existing ML approaches, as well as existing data-collection pipelines. The core issue in the described "trust problem" is simple: ML artifacts are unable to generalize beyond their training data set; the paper describes them as unaware of any potential biases that result from their training set. As a result, these ML artifacts fail in two key ways: there are "structural failure modes" where the model consistently provides an incorrect output (the paper calls this a misalignment between selected and desired predictors), then there are "underspecificed failure modes," where the training data doesn't contain info that pertains to a certain case, and the model produces unpredictable results. The paper also splits data collection into three components: development, deployment, and execution.

### Solution:
Unicorn is a platform that can simplify the data-collection process. The authors describe Unicorn as having three key features: reusability, evolvability, and scalability. Unicorn is intended to be reusable by providing a means through which data-collection pipelines can become isolated "tasks" that can be processed repeatedly and under different conditions. Another key part of Unicorn is its ability to isolate data from its infrastructure; each network system or provider may have a different infrastructure in which its data is stored, part of Unicorn's functionality is to be able to process and combine data from different infrastructure types so that it can all be used to gether to create a more complete model. 

### Unknowns:
Not much knowledge on concept of networking, nor what "problems" artifacts trained with unicorn-produced data are aiming to solve

## Reading Log Pass 2
### What is networking
https://www.geeksforgeeks.org/challenges-of-computer-network/

Networking refers to the process of connecting different computers together, as well as how they communicate between one another. Common networking problems have to do with network security (people sending harmful data or attempting to extract important data) and consistency (keeping network connections consistent).

### What is your takeaway message from this paper?
Unicorn is a tool to combine data collection pipelines to more effectively create training datasets for networking ML artifacts.

### What is the motivation for this work (both people and technical problem), and its distillation into a research question? Why doesn't the problem have a trivial solution? What are the previous solutions and why are they inadequate?
Networking is so large-scale, AI is powerful; we should apply AI to solve networking solutions and lower human costs. Existing AI artifacts have poor training data; they resultantly do not properly generalize to real-world scenarios. Existing methods of collecting data do not cover wide enough of a range of info to be effective.

### What is the proposed solution? Why is it believed it will work? How does it represent an improvement? How is the solution achieved?
Unicorn is a proposed system that allows for automated combining and formatting of different data pipelines to one uniform typing; that uniform database can then be selected from and modified again to fit different ML artifact needs. This will result in easier data collection (no hard coding for each pipeline) and collected data being usable across different artifacts.

### What is the author's evaluation of the solution? What logic, argument, evidence, artifacts (e.g., a proof-of-concept system), or experiments are presented in support of the idea?
ML artifact quality improves with quality of training data; the author claims that it is not the ML models that are insufficient, but rather that the data fed to train them is poor. As a result, any system that is capable of doing what the author proposes Unicorn will do must logically improve the overall networking ML artifact landscape. Although author does acknowlede the difficulties of implementing such a solution; their described prototype does yield statistical improvements (less lines of code having to be written per pipeline).

### What is your analysis of the identified problem, idea and evaluation? Is this a good idea? What flaws do you perceive in the work? What are the most interesting or controversial ideas? For work that has practical implications, ask whether this will work, who would want it, what it will take to give it to them, and when it might become a reality?
The idea itself seems good. Logically, the presented problem and proposed solution makes sense; if Unicorn is completed and commercialized, it would be realistic to see many internet providers and large software corporations adopt the technology to create models that better strengthen the security and stability of their internal computer networks (or in the case of internet providers, the networks they provide to ordinary people as a service).

### What are the paper's contributions (author's and your opinion)? Ideas, methods, software, experimental results, experimental techniques...?
The paper's biggest contribution seems to be the creation of a specification for a type of software; it proposes a specific style of solution to a larger problem that others may not have immediately thought of (I would have looked into model quality as opposed to data quality).

### What are the future directions for this research (author's and hours, perhaps driven by shortcomings or other critiques)?
Upon speaking to Professor Gupta, it feels like the research is leaning towards further improvements of the Unicorn prototype as well as other means of increasing overall trust in ML Artifacts, including a separate technology that involves converting some ML artifacts into visualizable decision trees (which seems extremely difficult but also very impressive)

### What questions are you left with? What questions would you like to raise in an open discussion of the work (review interesting and controversial points above)? What do you find difficult to understand? List as many as you can.
Overall remaining questions are mostly about the specifics in how the Unicorn prototype works, as well as what those data collection pipelines look like. So far, that information has not been easily findable online, will likely need Professor's or mentor's help in finding more info.

## Choosing teaching topic for Week 3
I'm leaning towards dynamic programming; I'm not exactly sure how it would fit into our project so far, but because DP is more problem-solving paradigm than specific methodology, I think it's helpful to learn anyway. Helps for interviews as well, which is a bonus.

# Week 1
Nothing here so far!
